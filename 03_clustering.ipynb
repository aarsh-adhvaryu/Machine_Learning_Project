{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9f1203a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================\n",
    "# PROJECT: Therapeutic Failure Phenotype Discovery\n",
    "# PHASE: Unsupervised Clustering (Phenotype Discovery)\n",
    "# FILE: 03_clustering_pipeline_minimal.py\n",
    "# --------------------------------------------------------------\n",
    "# FINAL VERSION — only essential outputs:\n",
    "#   • PROJECT_CLUSTERED_SEVERITY_DATA.csv\n",
    "#   • tfidf_vectorizer.joblib\n",
    "#   • truncated_svd.joblib\n",
    "#   • kmeans_failure_pheno.joblib\n",
    "# ==============================================================\n",
    "\n",
    "import os\n",
    "import time\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, ENGLISH_STOP_WORDS\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d03ebb12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------\n",
    "# 0) CONFIGURATION\n",
    "# --------------------------------------------------------------\n",
    "BASE_DIR = r\"D:\\ML_Project\"\n",
    "DATA_DIR = os.path.join(BASE_DIR, \"data\", \"processed\")\n",
    "MODELS_DIR = os.path.join(BASE_DIR, \"models\")\n",
    "LOG_DIR = os.path.join(BASE_DIR, \"logs\")\n",
    "\n",
    "SAMPLE_TRAIN_EVAL = os.path.join(DATA_DIR, \"sample_train_eval.csv\")\n",
    "OUT_CLUSTERED_FILE = os.path.join(DATA_DIR, \"PROJECT_CLUSTERED_SEVERITY_DATA.csv\")\n",
    "\n",
    "TFIDF_ARTIFACT = os.path.join(MODELS_DIR, \"tfidf_vectorizer.joblib\")\n",
    "SVD_ARTIFACT = os.path.join(MODELS_DIR, \"truncated_svd.joblib\")\n",
    "KMEANS_ARTIFACT = os.path.join(MODELS_DIR, \"kmeans_failure_pheno.joblib\")\n",
    "\n",
    "os.makedirs(MODELS_DIR, exist_ok=True)\n",
    "os.makedirs(LOG_DIR, exist_ok=True)\n",
    "\n",
    "TFIDF_PARAMS = {\n",
    "    \"max_features\": 2000,\n",
    "    \"ngram_range\": (1, 2),\n",
    "    \"min_df\": 20,\n",
    "    \"max_df\": 0.9,\n",
    "}\n",
    "SVD_COMPONENTS = 50\n",
    "K_RANGE = range(2, 7)\n",
    "FORCE_K = 3  # set None for auto select\n",
    "RANDOM_STATE = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ebf9bdc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading sample_train_eval...\n",
      "Loaded sample: 701,665 rows\n",
      "Failure rows for clustering: 249,048\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------------------\n",
    "# 1) LOAD SAMPLE (NOT FULL MASTER)\n",
    "# --------------------------------------------------------------\n",
    "start = time.time()\n",
    "print(\"Loading sample_train_eval...\")\n",
    "\n",
    "df = pd.read_csv(SAMPLE_TRAIN_EVAL, low_memory=False)\n",
    "print(f\"Loaded sample: {len(df):,} rows\")\n",
    "\n",
    "df_fail = df[df[\"is_failure\"] == 1].copy()\n",
    "print(f\"Failure rows for clustering: {len(df_fail):,}\")\n",
    "\n",
    "if len(df_fail) < 500:\n",
    "    raise ValueError(\"Too few failure rows. Sampling fraction too small.\")\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# 2) SEVERITY TAGGING (TEXT-BASED HEURISTIC)\n",
    "# --------------------------------------------------------------\n",
    "def categorize_severity(text: str) -> str:\n",
    "    t = str(text).lower()\n",
    "    critical = [\n",
    "        \"death\",\n",
    "        \"cardiac arrest\",\n",
    "        \"respiratory failure\",\n",
    "        \"shock\",\n",
    "        \"coma\",\n",
    "        \"multi organ failure\",\n",
    "    ]\n",
    "    hospital = [\n",
    "        \"hospital\",\n",
    "        \"emergency\",\n",
    "        \"infection\",\n",
    "        \"pneumonia\",\n",
    "        \"sepsis\",\n",
    "        \"treatment failure\",\n",
    "    ]\n",
    "    mild = [\"headache\", \"nausea\", \"vomiting\", \"rash\", \"fatigue\", \"dizziness\"]\n",
    "\n",
    "    if any(x in t for x in critical):\n",
    "        return \"Critical_Failure\"\n",
    "    if any(x in t for x in hospital):\n",
    "        return \"Hospitalization_Failure\"\n",
    "    if any(x in t for x in mild):\n",
    "        return \"SideEffect_Failure\"\n",
    "    return \"SideEffect_Failure\"\n",
    "\n",
    "\n",
    "df_fail[\"severity_category\"] = df_fail[\"all_reaction_pts\"].apply(categorize_severity)\n",
    "\n",
    "weight_map = {\n",
    "    \"Critical_Failure\": 3.0,\n",
    "    \"Hospitalization_Failure\": 2.0,\n",
    "    \"SideEffect_Failure\": 1.0,\n",
    "}\n",
    "df_fail[\"severity_weight\"] = df_fail[\"severity_category\"].map(weight_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9681073",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train failures: 199,238 | Holdout: 49,810\n",
      "Fitting TF-IDF...\n",
      "Applying SVD...\n",
      "\n",
      "Finding best K...\n",
      "K=2 → silhouette=0.3888\n",
      "K=3 → silhouette=0.4526\n",
      "K=4 → silhouette=0.3517\n",
      "K=5 → silhouette=0.3417\n",
      "K=6 → silhouette=0.3915\n",
      "Selected K = 3\n",
      "Fitting final KMeans...\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------------------\n",
    "# 3) TRAIN / HOLDOUT SPLIT\n",
    "# --------------------------------------------------------------\n",
    "train_df, holdout_df = train_test_split(\n",
    "    df_fail,\n",
    "    test_size=0.20,\n",
    "    random_state=RANDOM_STATE,\n",
    "    stratify=df_fail[\"severity_category\"],\n",
    ")\n",
    "\n",
    "print(f\"Train failures: {len(train_df):,} | Holdout: {len(holdout_df):,}\")\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# 4) TF-IDF + SVD\n",
    "# --------------------------------------------------------------\n",
    "custom_stopwords = sorted(\n",
    "    set(ENGLISH_STOP_WORDS).union(\n",
    "        {\n",
    "            \"drug\",\n",
    "            \"effect\",\n",
    "            \"reaction\",\n",
    "            \"report\",\n",
    "            \"adverse\",\n",
    "            \"therapy\",\n",
    "            \"patient\",\n",
    "            \"suspect\",\n",
    "            \"treatment\",\n",
    "        }\n",
    "    )\n",
    ")\n",
    "TFIDF_PARAMS[\"stop_words\"] = custom_stopwords\n",
    "\n",
    "print(\"Fitting TF-IDF...\")\n",
    "vectorizer = TfidfVectorizer(**TFIDF_PARAMS)\n",
    "X_train_tfidf = vectorizer.fit_transform(train_df[\"all_reaction_pts\"])\n",
    "X_holdout_tfidf = vectorizer.transform(holdout_df[\"all_reaction_pts\"])\n",
    "\n",
    "print(\"Applying SVD...\")\n",
    "svd = TruncatedSVD(n_components=SVD_COMPONENTS, random_state=RANDOM_STATE)\n",
    "X_train_red = svd.fit_transform(X_train_tfidf)\n",
    "X_holdout_red = svd.transform(X_holdout_tfidf)\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# 5) K SEARCH (FAST)\n",
    "# --------------------------------------------------------------\n",
    "print(\"\\nFinding best K...\")\n",
    "\n",
    "sil_scores = {}\n",
    "for k in K_RANGE:\n",
    "    km = KMeans(n_clusters=k, n_init=10, random_state=RANDOM_STATE)\n",
    "    labels = km.fit_predict(X_train_red)\n",
    "    sil_scores[k] = silhouette_score(X_train_red, labels)\n",
    "    print(f\"K={k} → silhouette={sil_scores[k]:.4f}\")\n",
    "\n",
    "best_k = FORCE_K if FORCE_K is not None else max(sil_scores, key=sil_scores.get)\n",
    "print(f\"Selected K = {best_k}\")\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# 6) FINAL KMEANS FIT + LABEL ASSIGNMENT\n",
    "# --------------------------------------------------------------\n",
    "print(\"Fitting final KMeans...\")\n",
    "kmeans = KMeans(n_clusters=best_k, n_init=20, random_state=RANDOM_STATE)\n",
    "\n",
    "train_labels = kmeans.fit_predict(X_train_red)\n",
    "holdout_labels = kmeans.predict(X_holdout_red)\n",
    "\n",
    "train_df[\"failure_phenotype\"] = train_labels\n",
    "holdout_df[\"failure_phenotype\"] = holdout_labels\n",
    "\n",
    "label_map = {i: f\"Cluster_{i}\" for i in range(best_k)}\n",
    "\n",
    "train_df[\"failure_phenotype_label\"] = train_df[\"failure_phenotype\"].map(label_map)\n",
    "holdout_df[\"failure_phenotype_label\"] = holdout_df[\"failure_phenotype\"].map(label_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34465caa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved → D:\\ML_Project\\data\\processed\\PROJECT_CLUSTERED_SEVERITY_DATA.csv\n",
      "Rows saved: 249,048\n",
      "\n",
      "Artifacts saved:\n",
      " - TF-IDF\n",
      " - SVD\n",
      " - KMeans\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------------------\n",
    "# 7) SAVE FINAL DATASET\n",
    "# --------------------------------------------------------------\n",
    "combined = pd.concat([train_df, holdout_df], ignore_index=True)\n",
    "combined.to_csv(OUT_CLUSTERED_FILE, index=False)\n",
    "\n",
    "print(f\"\\nSaved → {OUT_CLUSTERED_FILE}\")\n",
    "print(f\"Rows saved: {len(combined):,}\")\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# 8) SAVE ARTIFACTS\n",
    "# --------------------------------------------------------------\n",
    "joblib.dump(vectorizer, TFIDF_ARTIFACT)\n",
    "joblib.dump(svd, SVD_ARTIFACT)\n",
    "joblib.dump(kmeans, KMEANS_ARTIFACT)\n",
    "\n",
    "print(\"\\nArtifacts saved:\")\n",
    "print(\" - TF-IDF\")\n",
    "print(\" - SVD\")\n",
    "print(\" - KMeans\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f2390fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CLUSTERING SUMMARY\n",
      "===================\n",
      "Train failures: 199,238\n",
      "Holdout failures: 49,810\n",
      "Final K: 3\n",
      "Silhouette (train): 0.4526\n",
      "Silhouette (combined): 0.4517\n",
      "Runtime: 2391.68 sec\n",
      "\n",
      "\n",
      "Clustering pipeline complete.\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------------------\n",
    "# 9) SILHOUETTE REPORT\n",
    "# --------------------------------------------------------------\n",
    "sil_train = silhouette_score(X_train_red, train_labels)\n",
    "sil_combined = silhouette_score(\n",
    "    np.vstack([X_train_red, X_holdout_red]),\n",
    "    np.concatenate([train_labels, holdout_labels]),\n",
    ")\n",
    "\n",
    "summary = f\"\"\"\n",
    "CLUSTERING SUMMARY\n",
    "===================\n",
    "Train failures: {len(train_df):,}\n",
    "Holdout failures: {len(holdout_df):,}\n",
    "Final K: {best_k}\n",
    "Silhouette (train): {sil_train:.4f}\n",
    "Silhouette (combined): {sil_combined:.4f}\n",
    "Runtime: {round(time.time() - start, 2)} sec\n",
    "\"\"\"\n",
    "\n",
    "print(summary)\n",
    "with open(os.path.join(LOG_DIR, \"clustering_summary.txt\"), \"w\") as f:\n",
    "    f.write(summary)\n",
    "\n",
    "print(\"\\nClustering pipeline complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
