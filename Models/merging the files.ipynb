{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52e22339",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "data_path = r\"D:\\Python_practice\\group-project-ml\\data\\raw\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72f17c3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Processing DEMO files...\n",
      "  Processing: DEMO15Q1.txt...\n",
      "  Processing: DEMO15Q2.txt...\n",
      "  Processing: DEMO15Q3.txt...\n",
      "  Processing: DEMO15Q4.txt...\n",
      "  Processing: DEMO16Q1.txt...\n",
      "  Processing: DEMO16Q2.txt...\n",
      "  Processing: DEMO16Q3.txt...\n",
      "  Processing: DEMO16Q4.txt...\n",
      "  Processing: DEMO17Q1.txt...\n",
      "  Processing: DEMO17Q2.txt...\n",
      "  Processing: DEMO17Q3.txt...\n",
      "  Processing: DEMO17Q4.txt...\n",
      "  Processing: DEMO18Q1.txt...\n",
      "  Processing: DEMO18Q2.txt...\n",
      "  Processing: DEMO18Q3.txt...\n",
      "  Processing: DEMO18Q4.txt...\n",
      "  Processing: DEMO19Q1.txt...\n",
      "  Processing: DEMO19Q3.txt...\n",
      "  Processing: DEMO19Q4.txt...\n",
      "  Processing: DEMO20Q1.txt...\n",
      "  Processing: DEMO20Q2.txt...\n",
      "  Processing: DEMO20Q3.txt...\n",
      "  Processing: DEMO20Q4.txt...\n",
      "  Processing: DEMO21Q1.txt...\n",
      "  Processing: DEMO21Q2.txt...\n",
      "  Processing: DEMO21Q3.txt...\n",
      "  Processing: DEMO21Q4.txt...\n",
      "  Processing: DEMO22Q1.txt...\n",
      "  Processing: DEMO22Q2.txt...\n",
      "  Processing: DEMO22Q3.txt...\n",
      "  Processing: DEMO22Q4.txt...\n",
      "  Processing: DEMO23Q1.txt...\n",
      "  Processing: DEMO23Q2.txt...\n",
      "  Processing: DEMO23Q3.txt...\n",
      "  Processing: DEMO23Q4.txt...\n",
      "  Processing: DEMO24Q1.txt...\n",
      "  Processing: DEMO24Q2.txt...\n",
      "  Processing: DEMO24Q3.txt...\n",
      "  Processing: DEMO24Q4.txt...\n",
      "  Processing: DEMO25Q1.txt...\n",
      "  Processing: DEMO25Q2.txt...\n",
      "  Processing: DEMO25Q3.txt...\n",
      "Saved merged file: D:\\Python_practice\\group-project-ml\\data\\raw\\MASTER_DEMO.csv\n",
      "\n",
      "üîç Processing DRUG files...\n",
      "  Processing: DRUG15Q1.txt...\n",
      "  Processing: DRUG15Q2.txt...\n",
      "  Processing: DRUG15Q3.txt...\n",
      "  Processing: DRUG15Q4.txt...\n",
      "  Processing: DRUG16Q1.txt...\n",
      "  Processing: DRUG16Q2.txt...\n",
      "  Processing: DRUG16Q3.txt...\n",
      "  Processing: DRUG16Q4.txt...\n",
      "  Processing: DRUG17Q1.txt...\n",
      "  Processing: DRUG17Q2.txt...\n",
      "  Processing: DRUG17Q3.txt...\n",
      "  Processing: DRUG17Q4.txt...\n",
      "  Processing: DRUG18Q1.txt...\n",
      "  Processing: DRUG18Q2.txt...\n",
      "  Processing: DRUG18Q3.txt...\n",
      "  Processing: DRUG18Q4.txt...\n",
      "  Processing: DRUG19Q1.txt...\n",
      "  Processing: DRUG19Q3.txt...\n",
      "  ‚ùå Error processing DRUG19Q3.txt: 'utf-8' codec can't decode byte 0xad in position 3221: invalid start byte\n",
      "  Processing: DRUG19Q4.txt...\n",
      "  Processing: DRUG20Q1.txt...\n",
      "  Processing: DRUG20Q2.txt...\n",
      "  Processing: DRUG20Q3.txt...\n",
      "  Processing: DRUG20Q4.txt...\n",
      "  Processing: DRUG21Q1.txt...\n",
      "  Processing: DRUG21Q2.txt...\n",
      "  Processing: DRUG21Q3.txt...\n",
      "  Processing: DRUG21Q4.txt...\n",
      "  Processing: DRUG22Q1.txt...\n",
      "  Processing: DRUG22Q2.txt...\n",
      "  Processing: DRUG22Q3.txt...\n",
      "  Processing: DRUG22Q4.txt...\n",
      "  Processing: DRUG23Q1.txt...\n",
      "  Processing: DRUG23Q2.txt...\n",
      "  Processing: DRUG23Q3.txt...\n",
      "  Processing: DRUG23Q4.txt...\n",
      "  Processing: DRUG24Q1.txt...\n",
      "  Processing: DRUG24Q2.txt...\n",
      "  Processing: DRUG24Q3.txt...\n",
      "  Processing: DRUG24Q4.txt...\n",
      "  Processing: DRUG25Q1.txt...\n",
      "  Processing: DRUG25Q2.txt...\n",
      "  Processing: DRUG25Q3.txt...\n",
      "Saved merged file: D:\\Python_practice\\group-project-ml\\data\\raw\\MASTER_DRUG.csv\n",
      "\n",
      "üîç Processing INDI files...\n",
      "  Processing: INDI15Q1.txt...\n",
      "  Processing: INDI15Q2.txt...\n",
      "  Processing: INDI15Q3.txt...\n",
      "  Processing: INDI15Q4.txt...\n",
      "  Processing: INDI16Q1.txt...\n",
      "  Processing: INDI16Q2.txt...\n",
      "  Processing: INDI16Q3.txt...\n",
      "  Processing: INDI16Q4.txt...\n",
      "  Processing: INDI17Q1.txt...\n",
      "  Processing: INDI17Q2.txt...\n",
      "  Processing: INDI17Q3.txt...\n",
      "  Processing: INDI17Q4.txt...\n",
      "  Processing: INDI18Q1.txt...\n",
      "  Processing: INDI18Q2.txt...\n",
      "  Processing: INDI18Q3.txt...\n",
      "  Processing: INDI18Q4.txt...\n",
      "  Processing: INDI19Q1.txt...\n",
      "  Processing: INDI19Q3.txt...\n",
      "  Processing: INDI19Q4.txt...\n",
      "  Processing: INDI20Q1.txt...\n",
      "  Processing: INDI20Q2.txt...\n",
      "  Processing: INDI20Q3.txt...\n",
      "  Processing: INDI20Q4.txt...\n",
      "  Processing: INDI21Q1.txt...\n",
      "  Processing: INDI21Q2.txt...\n",
      "  Processing: INDI21Q3.txt...\n",
      "  Processing: INDI21Q4.txt...\n",
      "  Processing: INDI22Q1.txt...\n",
      "  Processing: INDI22Q2.txt...\n",
      "  Processing: INDI22Q3.txt...\n",
      "  Processing: INDI22Q4.txt...\n",
      "  Processing: INDI23Q1.txt...\n",
      "  Processing: INDI23Q2.txt...\n",
      "  Processing: INDI23Q3.txt...\n",
      "  Processing: INDI23Q4.txt...\n",
      "  Processing: INDI24Q1.txt...\n",
      "  Processing: INDI24Q2.txt...\n",
      "  Processing: INDI24Q3.txt...\n",
      "  Processing: INDI24Q4.txt...\n",
      "  Processing: INDI25Q1.txt...\n",
      "  Processing: INDI25Q2.txt...\n",
      "  Processing: INDI25Q3.txt...\n",
      "Saved merged file: D:\\Python_practice\\group-project-ml\\data\\raw\\MASTER_INDI.csv\n",
      "\n",
      "üîç Processing OUTC files...\n",
      "  Processing: OUTC15Q1.txt...\n",
      "  Processing: OUTC15Q2.txt...\n",
      "  Processing: OUTC15Q3.txt...\n",
      "  Processing: OUTC15Q4.txt...\n",
      "  Processing: OUTC16Q1.txt...\n",
      "  Processing: OUTC16Q2.txt...\n",
      "  Processing: OUTC16Q3.txt...\n",
      "  Processing: OUTC16Q4.txt...\n",
      "  Processing: OUTC17Q1.txt...\n",
      "  Processing: OUTC17Q2.txt...\n",
      "  Processing: OUTC17Q3.txt...\n",
      "  Processing: OUTC17Q4.txt...\n",
      "  Processing: OUTC18Q1.txt...\n",
      "  Processing: OUTC18Q2.txt...\n",
      "  Processing: OUTC18Q3.txt...\n",
      "  Processing: OUTC18Q4.txt...\n",
      "  Processing: OUTC19Q1.txt...\n",
      "  Processing: OUTC19Q3.txt...\n",
      "  Processing: OUTC19Q4.txt...\n",
      "  Processing: OUTC20Q1.txt...\n",
      "  Processing: OUTC20Q2.txt...\n",
      "  Processing: OUTC20Q3.txt...\n",
      "  Processing: OUTC20Q4.txt...\n",
      "  Processing: OUTC21Q1.txt...\n",
      "  Processing: OUTC21Q2.txt...\n",
      "  Processing: OUTC21Q3.txt...\n",
      "  Processing: OUTC21Q4.txt...\n",
      "  Processing: OUTC22Q1.txt...\n",
      "  Processing: OUTC22Q2.txt...\n",
      "  Processing: OUTC22Q3.txt...\n",
      "  Processing: OUTC22Q4.txt...\n",
      "  Processing: OUTC23Q1.txt...\n",
      "  Processing: OUTC23Q2.txt...\n",
      "  Processing: OUTC23Q3.txt...\n",
      "  Processing: OUTC23Q4.txt...\n",
      "  Processing: OUTC24Q1.txt...\n",
      "  Processing: OUTC24Q2.txt...\n",
      "  Processing: OUTC24Q3.txt...\n",
      "  Processing: OUTC24Q4.txt...\n",
      "  Processing: OUTC25Q1.txt...\n",
      "  Processing: OUTC25Q2.txt...\n",
      "  Processing: OUTC25Q3.txt...\n",
      "Saved merged file: D:\\Python_practice\\group-project-ml\\data\\raw\\MASTER_OUTC.csv\n",
      "\n",
      "üîç Processing REAC files...\n",
      "  Processing: REAC15Q1.txt...\n",
      "  Processing: REAC15Q2.txt...\n",
      "  Processing: REAC15Q3.txt...\n",
      "  Processing: REAC15Q4.txt...\n",
      "  Processing: REAC16Q1.txt...\n",
      "  Processing: REAC16Q2.txt...\n",
      "  Processing: REAC16Q3.txt...\n",
      "  Processing: REAC16Q4.txt...\n",
      "  Processing: REAC17Q1.txt...\n",
      "  Processing: REAC17Q2.txt...\n",
      "  Processing: REAC17Q3.txt...\n",
      "  Processing: REAC17Q4.txt...\n",
      "  Processing: REAC18Q1.txt...\n",
      "  Processing: REAC18Q2.txt...\n",
      "  Processing: REAC18Q3.txt...\n",
      "  Processing: REAC18Q4.txt...\n",
      "  Processing: REAC19Q1.txt...\n",
      "  Processing: REAC19Q3.txt...\n",
      "  Processing: REAC19Q4.txt...\n",
      "  Processing: REAC20Q1.txt...\n",
      "  Processing: REAC20Q2.txt...\n",
      "  Processing: REAC20Q3.txt...\n",
      "  Processing: REAC20Q4.txt...\n",
      "  Processing: REAC21Q1.txt...\n",
      "  Processing: REAC21Q2.txt...\n",
      "  Processing: REAC21Q3.txt...\n",
      "  Processing: REAC21Q4.txt...\n",
      "  Processing: REAC22Q1.txt...\n",
      "  Processing: REAC22Q2.txt...\n",
      "  Processing: REAC22Q3.txt...\n",
      "  Processing: REAC22Q4.txt...\n",
      "  Processing: REAC23Q1.txt...\n",
      "  Processing: REAC23Q2.txt...\n",
      "  Processing: REAC23Q3.txt...\n",
      "  Processing: REAC23Q4.txt...\n",
      "  Processing: REAC24Q1.txt...\n",
      "  Processing: REAC24Q2.txt...\n",
      "  Processing: REAC24Q3.txt...\n",
      "  Processing: REAC24Q4.txt...\n",
      "  Processing: REAC25Q1.txt...\n",
      "  Processing: REAC25Q2.txt...\n",
      "  Processing: REAC25Q3.txt...\n",
      "Saved merged file: D:\\Python_practice\\group-project-ml\\data\\raw\\MASTER_REAC.csv\n",
      "\n",
      "üîç Processing RPSR files...\n",
      "  Processing: RPSR15Q1.txt...\n",
      "  Processing: RPSR15Q2.txt...\n",
      "  Processing: RPSR15Q3.txt...\n",
      "  Processing: RPSR15Q4.txt...\n",
      "  Processing: RPSR16Q1.txt...\n",
      "  Processing: RPSR16Q2.txt...\n",
      "  Processing: RPSR16Q3.txt...\n",
      "  Processing: RPSR16Q4.txt...\n",
      "  Processing: RPSR17Q1.txt...\n",
      "  Processing: RPSR17Q2.txt...\n",
      "  Processing: RPSR17Q3.txt...\n",
      "  Processing: RPSR17Q4.txt...\n",
      "  Processing: RPSR18Q1.txt...\n",
      "  Processing: RPSR18Q2.txt...\n",
      "  Processing: RPSR18Q3.txt...\n",
      "  Processing: RPSR18Q4.txt...\n",
      "  Processing: RPSR19Q1.txt...\n",
      "  Processing: RPSR19Q3.txt...\n",
      "  Processing: RPSR19Q4.txt...\n",
      "  Processing: RPSR20Q1.txt...\n",
      "  Processing: RPSR20Q2.txt...\n",
      "  Processing: RPSR20Q3.txt...\n",
      "  Processing: RPSR20Q4.txt...\n",
      "  Processing: RPSR21Q1.txt...\n",
      "  Processing: RPSR21Q2.txt...\n",
      "  Processing: RPSR21Q3.txt...\n",
      "  Processing: RPSR21Q4.txt...\n",
      "  Processing: RPSR22Q1.txt...\n",
      "  Processing: RPSR22Q2.txt...\n",
      "  Processing: RPSR22Q3.txt...\n",
      "  Processing: RPSR22Q4.txt...\n",
      "  Processing: RPSR23Q1.txt...\n",
      "  Processing: RPSR23Q2.txt...\n",
      "  Processing: RPSR23Q3.txt...\n",
      "  Processing: RPSR23Q4.txt...\n",
      "  Processing: RPSR24Q1.txt...\n",
      "  Processing: RPSR24Q2.txt...\n",
      "  Processing: RPSR24Q3.txt...\n",
      "  Processing: RPSR24Q4.txt...\n",
      "  Processing: RPSR25Q1.txt...\n",
      "  Processing: RPSR25Q2.txt...\n",
      "  Processing: RPSR25Q3.txt...\n",
      "Saved merged file: D:\\Python_practice\\group-project-ml\\data\\raw\\MASTER_RPSR.csv\n",
      "\n",
      "üîç Processing THER files...\n",
      "  Processing: THER15Q1.txt...\n",
      "  Processing: THER15Q2.txt...\n",
      "  Processing: THER15Q3.txt...\n",
      "  Processing: THER15Q4.txt...\n",
      "  Processing: THER16Q1.txt...\n",
      "  Processing: THER16Q2.txt...\n",
      "  Processing: THER16Q3.txt...\n",
      "  Processing: THER16Q4.txt...\n",
      "  Processing: THER17Q1.txt...\n",
      "  Processing: THER17Q2.txt...\n",
      "  Processing: THER17Q3.txt...\n",
      "  Processing: THER17Q4.txt...\n",
      "  Processing: THER18Q1.txt...\n",
      "  Processing: THER18Q2.txt...\n",
      "  Processing: THER18Q3.txt...\n",
      "  Processing: THER18Q4.txt...\n",
      "  Processing: THER19Q1.txt...\n",
      "  Processing: THER19Q3.txt...\n",
      "  Processing: THER19Q4.txt...\n",
      "  Processing: THER20Q1.txt...\n",
      "  Processing: THER20Q2.txt...\n",
      "  Processing: THER20Q3.txt...\n",
      "  Processing: THER20Q4.txt...\n",
      "  Processing: THER21Q1.txt...\n",
      "  Processing: THER21Q2.txt...\n",
      "  Processing: THER21Q3.txt...\n",
      "  Processing: THER21Q4.txt...\n",
      "  Processing: THER22Q1.txt...\n",
      "  Processing: THER22Q2.txt...\n",
      "  Processing: THER22Q3.txt...\n",
      "  Processing: THER22Q4.txt...\n",
      "  Processing: THER23Q1.txt...\n",
      "  Processing: THER23Q2.txt...\n",
      "  Processing: THER23Q3.txt...\n",
      "  Processing: THER23Q4.txt...\n",
      "  Processing: THER24Q1.txt...\n",
      "  Processing: THER24Q2.txt...\n",
      "  Processing: THER24Q3.txt...\n",
      "  Processing: THER24Q4.txt...\n",
      "  Processing: THER25Q1.txt...\n",
      "  Processing: THER25Q2.txt...\n",
      "  Processing: THER25Q3.txt...\n",
      "Saved merged file: D:\\Python_practice\\group-project-ml\\data\\raw\\MASTER_THER.csv\n",
      "\n",
      "All processing complete.\n"
     ]
    }
   ],
   "source": [
    "file_types = [\"DEMO\", \"DRUG\", \"INDI\", \"OUTC\", \"REAC\", \"RPSR\", \"THER\"]\n",
    "years = range(2015, 2026)\n",
    "quarters = [\"Q1\", \"Q2\", \"Q3\", \"Q4\"]\n",
    "\n",
    "# (MASTER_SCHEMAS dictionary is the same as before, omitted for brevity)\n",
    "MASTER_SCHEMAS = {\n",
    "    \"DEMO\": {\n",
    "        \"columns\": [\n",
    "            \"primaryid\",\n",
    "            \"caseid\",\n",
    "            \"caseversion\",\n",
    "            \"i_f_code\",\n",
    "            \"event_dt\",\n",
    "            \"mfr_dt\",\n",
    "            \"init_fda_dt\",\n",
    "            \"fda_dt\",\n",
    "            \"rept_cod\",\n",
    "            \"auth_num\",\n",
    "            \"mfr_num\",\n",
    "            \"mfr_sndr\",\n",
    "            \"lit_ref\",\n",
    "            \"age\",\n",
    "            \"age_cod\",\n",
    "            \"age_grp\",\n",
    "            \"sex\",\n",
    "            \"e_sub\",\n",
    "            \"wt\",\n",
    "            \"wt_cod\",\n",
    "            \"rept_dt\",\n",
    "            \"to_mfr\",\n",
    "            \"occp_cod\",\n",
    "            \"reporter_country\",\n",
    "            \"occr_country\",\n",
    "        ],\n",
    "        \"rename_map\": {},\n",
    "    },\n",
    "    \"DRUG\": {\n",
    "        \"columns\": [\n",
    "            \"primaryid\",\n",
    "            \"caseid\",\n",
    "            \"drug_seq\",\n",
    "            \"role_cod\",\n",
    "            \"drugname\",\n",
    "            \"prod_ai\",\n",
    "            \"val_vbm\",\n",
    "            \"route\",\n",
    "            \"dose_vbm\",\n",
    "            \"cum_dose_chr\",\n",
    "            \"cum_dose_unit\",\n",
    "            \"dechal\",\n",
    "            \"rechal\",\n",
    "            \"lot_num\",\n",
    "            \"exp_dt\",\n",
    "            \"nda_num\",\n",
    "            \"dose_amt\",\n",
    "            \"dose_unit\",\n",
    "            \"dose_form\",\n",
    "            \"dose_freq\",\n",
    "        ],\n",
    "        \"rename_map\": {},\n",
    "    },\n",
    "    \"INDI\": {\n",
    "        \"columns\": [\"primaryid\", \"caseid\", \"indi_drug_seq\", \"indi_pt\"],\n",
    "        \"rename_map\": {},\n",
    "    },\n",
    "    \"OUTC\": {\"columns\": [\"primaryid\", \"caseid\", \"outc_cod\"], \"rename_map\": {}},\n",
    "    \"REAC\": {\n",
    "        \"columns\": [\"primaryid\", \"caseid\", \"pt\", \"drug_rec_act\"],\n",
    "        \"rename_map\": {},\n",
    "    },\n",
    "    \"RPSR\": {\"columns\": [\"primaryid\", \"caseid\", \"rpsr_cod\"], \"rename_map\": {}},\n",
    "    \"THER\": {\n",
    "        \"columns\": [\n",
    "            \"primaryid\",\n",
    "            \"caseid\",\n",
    "            \"dsg_drug_seq\",\n",
    "            \"start_dt\",\n",
    "            \"end_dt\",\n",
    "            \"dur\",\n",
    "            \"dur_cod\",\n",
    "        ],\n",
    "        \"rename_map\": {},\n",
    "    },\n",
    "}\n",
    "# --- End Configuration ---\n",
    "\n",
    "\n",
    "def process_and_align_file(filepath, master_columns, rename_map):\n",
    "    \"\"\"\n",
    "    Loads a single quarterly file, renames columns, and aligns\n",
    "    it to the master schema.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(\n",
    "            filepath, sep=\"$\", dtype=str, on_bad_lines=\"skip\", engine=\"python\"\n",
    "        )\n",
    "\n",
    "        df.columns = df.columns.str.lower()\n",
    "        df.rename(columns=rename_map, inplace=True)\n",
    "\n",
    "        extra_cols = set(df.columns) - set(master_columns)\n",
    "        if extra_cols:\n",
    "            df.drop(columns=list(extra_cols), inplace=True)\n",
    "\n",
    "        missing_cols = set(master_columns) - set(master_columns)\n",
    "        if missing_cols:\n",
    "            for col in missing_cols:\n",
    "                df[col] = pd.NA\n",
    "\n",
    "        df = df[master_columns]\n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ùå Error processing {os.path.basename(filepath)}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# --- Main Execution ---\n",
    "\n",
    "for ftype in file_types:\n",
    "    print(f\"\\nüîç Processing {ftype} files...\")\n",
    "\n",
    "    master_cols = MASTER_SCHEMAS[ftype][\"columns\"]\n",
    "    rename_map = MASTER_SCHEMAS[ftype][\"rename_map\"]\n",
    "\n",
    "    out_file = os.path.join(data_path, f\"MASTER_{ftype}.csv\")\n",
    "    is_first_file = True\n",
    "\n",
    "    for year in years:\n",
    "        for q in quarters:\n",
    "            if year == 2025 and q == \"Q4\":\n",
    "                break\n",
    "\n",
    "            pattern_upper = os.path.join(\n",
    "                data_path, f\"{ftype.upper()}{str(year)[-2:]}{q.upper()}.txt\"\n",
    "            )\n",
    "            matches = glob.glob(pattern_upper)\n",
    "\n",
    "            if not matches:\n",
    "                continue\n",
    "\n",
    "            for file in matches:\n",
    "                # This filter will correctly skip the PDFs\n",
    "                if not file.lower().endswith(\".txt\"):\n",
    "                    continue\n",
    "\n",
    "                print(f\"  Processing: {os.path.basename(file)}...\")\n",
    "\n",
    "                df = process_and_align_file(file, master_cols, rename_map)\n",
    "\n",
    "                if df is not None and not df.empty:\n",
    "                    if is_first_file:\n",
    "                        df.to_csv(out_file, index=False, mode=\"w\")\n",
    "                        is_first_file = False\n",
    "                    else:\n",
    "                        df.to_csv(out_file, index=False, mode=\"a\", header=False)\n",
    "\n",
    "    if is_first_file:\n",
    "        print(f\"  No valid .txt files found or processed for {ftype}\")\n",
    "    else:\n",
    "        print(f\"Saved merged file: {out_file}\")\n",
    "\n",
    "print(\"\\nAll processing complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d814e319",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading D:\\Python_practice\\group-project-ml\\data\\raw\\MASTER_DEMO.csv to check for duplicates...\n",
      "This may take a moment...\n",
      "\n",
      "Total records loaded: 16935987\n",
      "\n",
      "--- Duplicate Report ---\n",
      "üö® Found 1745993 unique cases that have duplicates (follow-ups).\n",
      "\n",
      "Top 50 most frequent cases (caseid: number_of_versions):\n",
      "caseid\n",
      "11020195    25\n",
      "12209653    22\n",
      "11692275    21\n",
      "9745027     21\n",
      "14698949    21\n",
      "7322498     20\n",
      "11565068    20\n",
      "9272239     20\n",
      "10637106    20\n",
      "9030666     20\n",
      "7056790     19\n",
      "14162985    19\n",
      "10338343    19\n",
      "12139469    19\n",
      "9383196     19\n",
      "14719399    18\n",
      "10990475    18\n",
      "10231642    18\n",
      "15775809    18\n",
      "12122305    18\n",
      "12612206    18\n",
      "7059978     18\n",
      "9368878     18\n",
      "11090837    18\n",
      "12943377    18\n",
      "12220151    18\n",
      "10720490    18\n",
      "9695444     18\n",
      "10982063    18\n",
      "12913352    18\n",
      "10428818    18\n",
      "12304566    18\n",
      "9459293     18\n",
      "11687892    18\n",
      "14899555    18\n",
      "17829120    18\n",
      "11072666    18\n",
      "16215378    18\n",
      "17440764    18\n",
      "11663523    18\n",
      "8244014     18\n",
      "14752295    18\n",
      "8303475     17\n",
      "9869027     17\n",
      "8203402     17\n",
      "17332255    17\n",
      "18204396    17\n",
      "15759396    17\n",
      "13648277    17\n",
      "14680853    17\n",
      "Name: count, dtype: int64\n",
      "\n",
      "--- Summary ---\n",
      "Total rows in your file:  16935987\n",
      "Unique patient stories: 14557323\n",
      "Total rows that are duplicates: 2378664\n"
     ]
    }
   ],
   "source": [
    "MASTER_DEMO_FILE = r\"D:\\Python_practice\\group-project-ml\\data\\raw\\MASTER_DEMO.csv\"\n",
    "# ---------------------\n",
    "\n",
    "print(f\"Loading {MASTER_DEMO_FILE} to check for duplicates...\")\n",
    "print(\"This may take a moment...\")\n",
    "\n",
    "try:\n",
    "    # Load only the 'caseid' column to save memory\n",
    "    df_demo = pd.read_csv(MASTER_DEMO_FILE, usecols=[\"caseid\"], dtype={\"caseid\": \"str\"})\n",
    "\n",
    "    print(f\"\\nTotal records loaded: {len(df_demo)}\")\n",
    "\n",
    "    # --- The Check You Suggested ---\n",
    "\n",
    "    # 1. Count how many times each caseid appears\n",
    "    caseid_counts = df_demo[\"caseid\"].value_counts()\n",
    "\n",
    "    # 2. Filter this list to show only the duplicates\n",
    "    #    (where the count is greater than 1)\n",
    "    duplicate_caseids = caseid_counts[caseid_counts > 1]\n",
    "\n",
    "    # --- Report the Findings ---\n",
    "\n",
    "    print(\"\\n--- Duplicate Report ---\")\n",
    "    if duplicate_caseids.empty:\n",
    "        print(\"‚úÖ No duplicate caseids found!\")\n",
    "    else:\n",
    "        print(\n",
    "            f\"üö® Found {len(duplicate_caseids)} unique cases that have duplicates (follow-ups).\"\n",
    "        )\n",
    "        print(\"\\nTop 50 most frequent cases (caseid: number_of_versions):\")\n",
    "\n",
    "        # Sort by count (descending) and print the top 50\n",
    "        print(duplicate_caseids.sort_values(ascending=False).head(50))\n",
    "\n",
    "        total_duplicate_rows = duplicate_caseids.sum()\n",
    "        unique_rows = len(caseid_counts)\n",
    "        print(\"\\n--- Summary ---\")\n",
    "        print(f\"Total rows in your file:  {len(df_demo)}\")\n",
    "        print(f\"Unique patient stories: {unique_rows}\")\n",
    "        print(\n",
    "            f\"Total rows that are duplicates: {total_duplicate_rows - len(duplicate_caseids)}\"\n",
    "        )\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: File not found at {MASTER_DEMO_FILE}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d3088d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting deduplication...\n",
      "Loaded 16935987 records from D:\\Python_practice\\group-project-ml\\data\\raw\\MASTER_DEMO.csv\n",
      "Sorting records to find the latest versions...\n",
      "\n",
      "Original records: 16935987\n",
      "Unique reports found: 14557323\n",
      "‚úÖ Successfully saved unique reports to D:\\Python_practice\\group-project-ml\\data\\processed\\UNIQUE_DEMO.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "print(\"Starting deduplication...\")\n",
    "\n",
    "# --- Configuration ---\n",
    "# ‚ö†Ô∏è Update these paths\n",
    "MASTER_DEMO_FILE = r\"D:\\Python_practice\\group-project-ml\\data\\raw\\MASTER_DEMO.csv\"\n",
    "UNIQUE_DEMO_FILE = r\"D:\\Python_practice\\group-project-ml\\data\\processed\\UNIQUE_DEMO.csv\"\n",
    "# ---------------------\n",
    "\n",
    "# Load the master demo file\n",
    "try:\n",
    "    df_demo = pd.read_csv(\n",
    "        MASTER_DEMO_FILE,\n",
    "        dtype=str,  # Load all as strings to avoid date/number issues\n",
    "        low_memory=False,\n",
    "    )\n",
    "    print(f\"Loaded {len(df_demo)} records from {MASTER_DEMO_FILE}\")\n",
    "\n",
    "    # --- Deduplication Logic ---\n",
    "\n",
    "    # 1. Convert date columns for sorting.\n",
    "    #    'fda_dt' is for the latest version of the report.\n",
    "    df_demo[\"fda_dt\"] = pd.to_datetime(\n",
    "        df_demo[\"fda_dt\"], format=\"%Y%m%d\", errors=\"coerce\"\n",
    "    )\n",
    "\n",
    "    # 2. Convert key IDs to numeric for correct sorting\n",
    "    df_demo[\"primaryid\"] = pd.to_numeric(df_demo[\"primaryid\"], errors=\"coerce\")\n",
    "    df_demo[\"caseid\"] = pd.to_numeric(df_demo[\"caseid\"], errors=\"coerce\")\n",
    "\n",
    "    # Handle any conversion errors (rows with bad data)\n",
    "    df_demo.dropna(subset=[\"caseid\", \"fda_dt\", \"primaryid\"], inplace=True)\n",
    "\n",
    "    print(\"Sorting records to find the latest versions...\")\n",
    "\n",
    "    # 3. Sort by Case ID, then newest date, then highest primaryid\n",
    "    df_demo.sort_values(\n",
    "        by=[\"caseid\", \"fda_dt\", \"primaryid\"],\n",
    "        ascending=[True, False, False],  # caseid (asc), fda_dt (desc), primaryid (desc)\n",
    "        inplace=True,\n",
    "    )\n",
    "\n",
    "    # 4. Keep only the FIRST row for each 'caseid'. This is the latest report.\n",
    "    df_deduplicated = df_demo.drop_duplicates(subset=[\"caseid\"], keep=\"first\")\n",
    "\n",
    "    print(f\"\\nOriginal records: {len(df_demo)}\")\n",
    "    print(\n",
    "        f\"Unique reports found: {len(df_deduplicated)}\"\n",
    "    )  # This should match your 'Unique patient stories' number\n",
    "\n",
    "    # 5. Save the unique, deduplicated reports\n",
    "    df_deduplicated.to_csv(UNIQUE_DEMO_FILE, index=False)\n",
    "    print(f\"‚úÖ Successfully saved unique reports to {UNIQUE_DEMO_FILE}\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: File not found at {MASTER_DEMO_FILE}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8647ef5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting creation of FINAL_MASTER_DATASET.csv...\n",
      "\n",
      "--- Phase 1: Aggregating Relational Files ---\n",
      "  Processing Outcomes (OUTC)...\n",
      "    Done. Found 9693475 aggregated outcomes.\n",
      "  Processing Drugs (DRUG)...\n",
      "    Done. Found 16482610 aggregated drug counts.\n",
      "  Processing Indications (INDI)...\n",
      "    Done. Found 15544380 aggregated indication counts.\n",
      "  Processing Reactions (REAC)...\n",
      "    Done. Found 16935474 aggregated reaction features.\n",
      "  Processing Therapy (THER)...\n",
      "    Done. Found 82794 aggregated therapy durations.\n",
      "\n",
      "--- Phase 2: Loading Base `UNIQUE_DEMO` File ---\n",
      "Loaded 14557323 unique reports from UNIQUE_DEMO.csv\n",
      "\n",
      "--- Phase 3: Joining All DataFrames ---\n",
      "  All joins complete.\n",
      "\n",
      "--- Phase 4: Final Cleaning and Saving ---\n",
      "  Created final 'is_failure' target column.\n",
      "\n",
      "üéâ SUCCESS! üéâ\n",
      "Final flattened dataset saved to: D:\\Python_practice\\group-project-ml\\data\\processed\\FINAL_MASTER_DATASET.csv\n",
      "It contains 14557323 rows and 32 columns.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "print(\"üöÄ Starting creation of FINAL_MASTER_DATASET.csv...\")\n",
    "\n",
    "# --- 1. Configuration: UPDATE ALL FILE PATHS ---\n",
    "\n",
    "# Base file (the 14.5M unique reports)\n",
    "UNIQUE_DEMO_FILE = r\"D:\\Python_practice\\group-project-ml\\data\\processed\\UNIQUE_DEMO.csv\"\n",
    "\n",
    "\n",
    "# Source master files (the ones you already built)\n",
    "MASTER_OUTC_FILE = r\"D:\\Python_practice\\group-project-ml\\data\\raw\\MASTER_OUTC.csv\"\n",
    "MASTER_DRUG_FILE = r\"D:\\Python_practice\\group-project-ml\\data\\raw\\MASTER_DRUG.csv\"\n",
    "MASTER_REAC_FILE = r\"D:\\Python_practice\\group-project-ml\\data\\raw\\MASTER_REAC.csv\"\n",
    "MASTER_INDI_FILE = r\"D:\\Python_practice\\group-project-ml\\data\\raw\\MASTER_INDI.csv\"\n",
    "MASTER_THER_FILE = r\"D:\\Python_practice\\group-project-ml\\data\\raw\\MASTER_THER.csv\"\n",
    "\n",
    "# Final output file\n",
    "FINAL_DATASET_FILE = r\"D:\\Python_practice\\group-project-ml\\data\\processed\\FINAL_MASTER_DATASET.csv\"\n",
    "\n",
    "# --- 2. Define Project Lists ---\n",
    "\n",
    "# List of \"Severe\" outcomes for Project 1's target\n",
    "SEVERE_OUTCOMES = [\"DE\", \"LT\", \"HO\", \"DS\"]\n",
    "\n",
    "# List of \"Inefficacy\" terms for Project 2's target\n",
    "# We must use uppercase for matching\n",
    "INEFFICACY_TERMS = [\n",
    "    \"DRUG INEFFECTIVE\",\n",
    "    \"LACK OF EFFECT\",\n",
    "    \"THERAPEUTIC RESPONSE DECREASED\",\n",
    "    \"CONDITION WORSENED\",\n",
    "]\n",
    "\n",
    "# --- 3. Feature Engineering Functions ---\n",
    "\n",
    "\n",
    "def process_outcomes(filepath):\n",
    "    print(\"  Processing Outcomes (OUTC)...\")\n",
    "    try:\n",
    "        df = pd.read_csv(filepath, dtype=str, usecols=[\"primaryid\", \"outc_cod\"])\n",
    "        df.dropna(inplace=True)\n",
    "\n",
    "        # Mark '1' if the outcome is in our severe list\n",
    "        df[\"is_severe_num\"] = df[\"outc_cod\"].isin(SEVERE_OUTCOMES).astype(int)\n",
    "\n",
    "        # Aggregate: Group by primaryid and take the MAX.\n",
    "        # (If any outcome is severe, the whole report is severe)\n",
    "        df_agg = (\n",
    "            df.groupby(\"primaryid\")[\"is_severe_num\"].max().to_frame(\"is_severe_outcome\")\n",
    "        )\n",
    "\n",
    "        print(f\"    Done. Found {len(df_agg)} aggregated outcomes.\")\n",
    "        return df_agg\n",
    "    except FileNotFoundError:\n",
    "        print(f\"    ERROR: OUTC file not found at {filepath}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def process_drugs(filepath):\n",
    "    print(\"  Processing Drugs (DRUG)...\")\n",
    "    try:\n",
    "        df = pd.read_csv(filepath, dtype=str, usecols=[\"primaryid\"])\n",
    "\n",
    "        # Aggregate: Just count the number of rows per primaryid\n",
    "        df_agg = df.groupby(\"primaryid\").size().to_frame(\"drug_count\")\n",
    "\n",
    "        print(f\"    Done. Found {len(df_agg)} aggregated drug counts.\")\n",
    "        return df_agg\n",
    "    except FileNotFoundError:\n",
    "        print(f\"    ERROR: DRUG file not found at {filepath}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def process_indications(filepath):\n",
    "    print(\"  Processing Indications (INDI)...\")\n",
    "    try:\n",
    "        df = pd.read_csv(filepath, dtype=str, usecols=[\"primaryid\"])\n",
    "\n",
    "        # Aggregate: Count number of indications per primaryid\n",
    "        df_agg = df.groupby(\"primaryid\").size().to_frame(\"indication_count\")\n",
    "\n",
    "        print(f\"    Done. Found {len(df_agg)} aggregated indication counts.\")\n",
    "        return df_agg\n",
    "    except FileNotFoundError:\n",
    "        print(f\"    ERROR: INDI file not found at {filepath}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def process_reactions(filepath):\n",
    "    print(\"  Processing Reactions (REAC)...\")\n",
    "    try:\n",
    "        df = pd.read_csv(filepath, dtype=str, usecols=[\"primaryid\", \"pt\"])\n",
    "        df.dropna(inplace=True)\n",
    "\n",
    "        # Standardize: Make all reaction terms uppercase for matching\n",
    "        df[\"pt_upper\"] = df[\"pt\"].str.upper()\n",
    "\n",
    "        # Aggregate 1: Count reactions\n",
    "        agg_count = df.groupby(\"primaryid\").size().to_frame(\"reaction_count\")\n",
    "\n",
    "        # Aggregate 2: Create 'all_reaction_pts' for Project 2 clustering\n",
    "        agg_pts = (\n",
    "            df.groupby(\"primaryid\")[\"pt_upper\"]\n",
    "            .apply(\" \".join)\n",
    "            .to_frame(\"all_reaction_pts\")\n",
    "        )\n",
    "\n",
    "        # Aggregate 3: Create 'is_ineffective' for Project 2 target\n",
    "        def check_ineffective(x):\n",
    "            return any(term in INEFFICACY_TERMS for term in x)\n",
    "\n",
    "        agg_ineffective = (\n",
    "            df.groupby(\"primaryid\")[\"pt_upper\"]\n",
    "            .apply(check_ineffective)\n",
    "            .astype(int)\n",
    "            .to_frame(\"is_ineffective\")\n",
    "        )\n",
    "\n",
    "        # Merge the 3 aggregated DFs\n",
    "        df_agg = agg_count.join(agg_pts, how=\"outer\").join(agg_ineffective, how=\"outer\")\n",
    "\n",
    "        print(f\"    Done. Found {len(df_agg)} aggregated reaction features.\")\n",
    "        return df_agg\n",
    "    except FileNotFoundError:\n",
    "        print(f\"    ERROR: REAC file not found at {filepath}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def process_therapy(filepath):\n",
    "    print(\"  Processing Therapy (THER)...\")\n",
    "    try:\n",
    "        df = pd.read_csv(filepath, dtype=str, usecols=[\"primaryid\", \"dur\", \"dur_cod\"])\n",
    "        df.dropna(inplace=True)\n",
    "\n",
    "        # Convert duration to a number\n",
    "        df[\"dur\"] = pd.to_numeric(df[\"dur\"], errors=\"coerce\")\n",
    "        df.dropna(subset=[\"dur\"], inplace=True)\n",
    "\n",
    "        # Map codes to day multipliers\n",
    "        duration_map = {\n",
    "            \"DY\": 1,\n",
    "            \"WK\": 7,\n",
    "            \"MO\": 30.4375,  # Avg days in month\n",
    "            \"YR\": 365.25,  # Account for leap year\n",
    "        }\n",
    "\n",
    "        df[\"multiplier\"] = df[\"dur_cod\"].map(duration_map)\n",
    "        df.dropna(subset=[\"multiplier\"], inplace=True)\n",
    "\n",
    "        # Calculate duration in days\n",
    "        df[\"duration_in_days\"] = df[\"dur\"] * df[\"multiplier\"]\n",
    "\n",
    "        # Aggregate: Find the *maximum* therapy duration reported for a case\n",
    "        df_agg = (\n",
    "            df.groupby(\"primaryid\")[\"duration_in_days\"]\n",
    "            .max()\n",
    "            .to_frame(\"therapy_duration_days\")\n",
    "        )\n",
    "\n",
    "        print(f\"    Done. Found {len(df_agg)} aggregated therapy durations.\")\n",
    "        return df_agg\n",
    "    except FileNotFoundError:\n",
    "        print(f\"    ERROR: THER file not found at {filepath}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# --- 4. Main Execution ---\n",
    "\n",
    "print(\"\\n--- Phase 1: Aggregating Relational Files ---\")\n",
    "df_outc = process_outcomes(MASTER_OUTC_FILE)\n",
    "df_drug = process_drugs(MASTER_DRUG_FILE)\n",
    "df_indi = process_indications(MASTER_INDI_FILE)\n",
    "df_reac = process_reactions(MASTER_REAC_FILE)\n",
    "df_ther = process_therapy(MASTER_THER_FILE)\n",
    "\n",
    "# Create a list of all aggregated dataframes that exist\n",
    "aggregated_dfs = [df_outc, df_drug, df_indi, df_reac, df_ther]\n",
    "aggregated_dfs = [df for df in aggregated_dfs if df is not None]\n",
    "\n",
    "print(\"\\n--- Phase 2: Loading Base `UNIQUE_DEMO` File ---\")\n",
    "try:\n",
    "    df_base = pd.read_csv(UNIQUE_DEMO_FILE, dtype=str)\n",
    "    # Set primaryid as index for efficient joining\n",
    "    df_base[\"primaryid\"] = pd.to_numeric(df_base[\"primaryid\"], errors=\"coerce\")\n",
    "    df_base.set_index(\"primaryid\", inplace=True)\n",
    "    print(f\"Loaded {len(df_base)} unique reports from UNIQUE_DEMO.csv\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"FATAL ERROR: UNIQUE_DEMO_FILE not found at {UNIQUE_DEMO_FILE}\")\n",
    "    exit()\n",
    "\n",
    "print(\"\\n--- Phase 3: Joining All DataFrames ---\")\n",
    "# Make sure aggregated DFs have the same index type\n",
    "for df in aggregated_dfs:\n",
    "    df.index = pd.to_numeric(df.index, errors=\"coerce\")\n",
    "\n",
    "# Join all aggregated DFs to the base demo file\n",
    "df_final = df_base.join(aggregated_dfs, how=\"left\")\n",
    "\n",
    "print(\"  All joins complete.\")\n",
    "\n",
    "print(\"\\n--- Phase 4: Final Cleaning and Saving ---\")\n",
    "\n",
    "# Fill NaNs created by joins\n",
    "# 'count' features should be 0 (0 drugs, 0 reactions)\n",
    "count_cols = [\"drug_count\", \"indication_count\", \"reaction_count\"]\n",
    "for col in count_cols:\n",
    "    if col in df_final.columns:\n",
    "        df_final[col] = df_final[col].fillna(0).astype(int)\n",
    "\n",
    "# 'outcome' features should be 0 (not severe, not ineffective)\n",
    "outcome_cols = [\"is_severe_outcome\", \"is_ineffective\"]\n",
    "for col in outcome_cols:\n",
    "    if col in df_final.columns:\n",
    "        df_final[col] = df_final[col].fillna(0).astype(int)\n",
    "\n",
    "# 'text' features should be empty string\n",
    "if \"all_reaction_pts\" in df_final.columns:\n",
    "    df_final[\"all_reaction_pts\"] = df_final[\"all_reaction_pts\"].fillna(\"\")\n",
    "\n",
    "# 'therapy_duration_days' should stay as NaN (it's a regression target)\n",
    "\n",
    "# Create the final target for Project 2: 'is_failure'\n",
    "if \"is_severe_outcome\" in df_final.columns and \"is_ineffective\" in df_final.columns:\n",
    "    df_final[\"is_failure\"] = (\n",
    "        (df_final[\"is_severe_outcome\"] == 1) | (df_final[\"is_ineffective\"] == 1)\n",
    "    ).astype(int)\n",
    "    print(\"  Created final 'is_failure' target column.\")\n",
    "\n",
    "# Save the final dataset\n",
    "df_final.to_csv(FINAL_DATASET_FILE)\n",
    "\n",
    "print(f\"\\nüéâ SUCCESS! üéâ\\nFinal flattened dataset saved to: {FINAL_DATASET_FILE}\")\n",
    "print(f\"It contains {len(df_final)} rows and {len(df_final.columns)} columns.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
