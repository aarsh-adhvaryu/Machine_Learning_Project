{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "20317e36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded preprocessing + balancing script.\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================\n",
    "# PROJECT: Therapeutic Failure Phenotype Discovery\n",
    "# PHASE: Classification Data Prep + Multi-Class Balancing\n",
    "# ==============================================================\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# PATHS\n",
    "# --------------------------------------------------------------\n",
    "BASE_DIR   = r\"D:\\ML_Project\"\n",
    "DATA_PATH  = os.path.join(BASE_DIR, \"data\", \"processed\", \"PROJECT_CLUSTERED_SEVERITY_DATA.csv\")\n",
    "MODELS_DIR = os.path.join(BASE_DIR, \"models\")\n",
    "\n",
    "os.makedirs(MODELS_DIR, exist_ok=True)\n",
    "\n",
    "print(\"Loaded preprocessing + balancing script.\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3c4d5b84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded rows: 249048\n",
      "Final feature frame: (249048, 21)\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------------------\n",
    "# 1) LOAD DATA\n",
    "# --------------------------------------------------------------\n",
    "df = pd.read_csv(DATA_PATH, low_memory=False)\n",
    "print(\"Loaded rows:\", len(df))\n",
    "\n",
    "# Convert clusters → phenotype\n",
    "cluster_map = {\n",
    "    \"Cluster_0\": \"Critical_Failure\",\n",
    "    \"Cluster_1\": \"Hospitalization_Failure\",\n",
    "    \"Cluster_2\": \"SideEffect_Failure\"\n",
    "}\n",
    "df[\"failure_phenotype_label\"] = df[\"failure_phenotype_label\"].replace(cluster_map)\n",
    "\n",
    "# Drop useless columns\n",
    "drop_cols = [\n",
    "    \"primaryid\",\"caseid\",\"caseversion\",\"fda_dt_parsed\",\n",
    "    \"severity_category\",\"severity_weight\",\n",
    "    \"failure_phenotype\",\"is_failure\",\n",
    "    \"all_reaction_pts\"\n",
    "]\n",
    "df.drop(columns=[c for c in drop_cols if c in df.columns], inplace=True)\n",
    "\n",
    "print(\"Final feature frame:\", df.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "75d2bd45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved training_feature_names.joblib\n",
      "Saved label_encoder.joblib\n",
      "Saved scaler.joblib\n",
      "Saved X_test.joblib & y_test.joblib\n",
      "Train: (199238, 20) Test: (49810, 20)\n",
      "\n",
      "Applying multi-class SMOTE + undersampling...\n",
      "Original class distribution: {np.int64(0): np.int64(169291), np.int64(1): np.int64(14130), np.int64(2): np.int64(15817)}\n",
      "SMOTE strategy: {np.int64(0): np.int64(169291), np.int64(1): np.int64(169291), np.int64(2): np.int64(169291)}\n",
      "Under-sampling strategy: {np.int64(0): 143897, np.int64(1): 143897, np.int64(2): 143897}\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------------------\n",
    "# 2) DEFINE X, y\n",
    "# --------------------------------------------------------------\n",
    "X = df.drop(columns=[\"failure_phenotype_label\"])\n",
    "y = df[\"failure_phenotype_label\"]\n",
    "\n",
    "joblib.dump(X.columns.tolist(), os.path.join(MODELS_DIR, \"training_feature_names.joblib\"))\n",
    "print(\"Saved training_feature_names.joblib\")\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# 3) LABEL ENCODE TARGET\n",
    "# --------------------------------------------------------------\n",
    "le = LabelEncoder()\n",
    "y_enc = le.fit_transform(y)\n",
    "joblib.dump(le, os.path.join(MODELS_DIR, \"label_encoder.joblib\"))\n",
    "print(\"Saved label_encoder.joblib\")\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# 4) SCALE NUMERIC FEATURES\n",
    "# --------------------------------------------------------------\n",
    "num_cols = [c for c in X.columns if np.issubdtype(X[c].dtype, np.number)]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X[num_cols] = scaler.fit_transform(X[num_cols])\n",
    "\n",
    "joblib.dump(scaler, os.path.join(MODELS_DIR, \"scaler.joblib\"))\n",
    "print(\"Saved scaler.joblib\")\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# 5) TRAIN/TEST SPLIT\n",
    "# --------------------------------------------------------------\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_enc, test_size=0.2, stratify=y_enc, random_state=42\n",
    ")\n",
    "\n",
    "joblib.dump(X_test, os.path.join(MODELS_DIR, \"X_test.joblib\"))\n",
    "joblib.dump(y_test, os.path.join(MODELS_DIR, \"y_test.joblib\"))\n",
    "\n",
    "print(\"Saved X_test.joblib & y_test.joblib\")\n",
    "print(\"Train:\", X_train.shape, \"Test:\", X_test.shape)\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# 6) MULTI-CLASS BALANCING (Correct SMOTE + UnderSample)\n",
    "# --------------------------------------------------------------\n",
    "print(\"\\nApplying multi-class SMOTE + undersampling...\")\n",
    "\n",
    "# Count classes\n",
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "class_counts = dict(zip(unique, counts))\n",
    "print(\"Original class distribution:\", class_counts)\n",
    "\n",
    "# --------------------\n",
    "# SMOTE Strategy\n",
    "# Oversample all classes to the size of the largest class.\n",
    "# --------------------\n",
    "max_target = max(class_counts.values())\n",
    "\n",
    "smote_strategy = {cls: max_target for cls in class_counts.keys()}\n",
    "print(\"SMOTE strategy:\", smote_strategy)\n",
    "\n",
    "oversample = SMOTE(\n",
    "    sampling_strategy=smote_strategy,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# --------------------\n",
    "# Under-sample Strategy:\n",
    "# Reduce ALL classes to 85% of max size (prevents overfitting)\n",
    "# --------------------\n",
    "under_strategy = {cls: int(max_target * 0.85) for cls in class_counts.keys()}\n",
    "print(\"Under-sampling strategy:\", under_strategy)\n",
    "\n",
    "undersample = RandomUnderSampler(\n",
    "    sampling_strategy=under_strategy,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8cf03355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced class distribution: {np.int64(0): np.int64(143897), np.int64(1): np.int64(143897), np.int64(2): np.int64(143897)}\n",
      "Balanced shapes: (431691, 20) (431691,)\n",
      "\n",
      "Balanced training data saved:\n",
      " → X_train_bal.joblib\n",
      " → y_train_bal.joblib\n",
      "\n",
      "PREPROCESSING + BALANCING COMPLETE.\n"
     ]
    }
   ],
   "source": [
    "# --------------------\n",
    "# Pipeline\n",
    "# --------------------\n",
    "balance_pipe = Pipeline([\n",
    "    (\"smote\", oversample),\n",
    "    (\"under\", undersample)\n",
    "])\n",
    "\n",
    "X_train_bal, y_train_bal = balance_pipe.fit_resample(X_train, y_train)\n",
    "\n",
    "print(\"Balanced class distribution:\", dict(zip(*np.unique(y_train_bal, return_counts=True))))\n",
    "print(\"Balanced shapes:\", X_train_bal.shape, y_train_bal.shape)\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# SAVE BALANCED DATA\n",
    "# --------------------------------------------------------------\n",
    "joblib.dump(X_train_bal, os.path.join(MODELS_DIR, \"X_train_bal.joblib\"))\n",
    "joblib.dump(y_train_bal, os.path.join(MODELS_DIR, \"y_train_bal.joblib\"))\n",
    "\n",
    "print(\"\\nBalanced training data saved:\")\n",
    "print(\" → X_train_bal.joblib\")\n",
    "print(\" → y_train_bal.joblib\")\n",
    "\n",
    "print(\"\\nPREPROCESSING + BALANCING COMPLETE.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "29e2547b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "=== RUNNING UNTUNED BASELINE MODELS (for selection) ===\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Training Untuned Model → Logistic Regression\n",
      "Untuned F1: 0.7684 | Acc: 0.8225 | PR-AUC: 0.8059\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Training Untuned Model → Decision Tree\n",
      "Untuned F1: 0.8012 | Acc: 0.8876 | PR-AUC: 0.7446\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Training Untuned Model → Random Forest\n",
      "Untuned F1: 0.8079 | Acc: 0.8901 | PR-AUC: 0.8048\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Training Untuned Model → XGBoost\n",
      "Untuned F1: 0.8068 | Acc: 0.8757 | PR-AUC: 0.8394\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Training Untuned Model → LightGBM\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 1059\n",
      "[LightGBM] [Info] Number of data points in the train set: 431691, number of used features: 20\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 5070 Ti Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 7 dense feature groups (3.29 MB) transferred to GPU in 0.002528 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "Untuned F1: 0.8064 | Acc: 0.8765 | PR-AUC: 0.8372\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Model",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "F1",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Accuracy",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "PR-AUC",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "cbef76ae-c8cd-4bf0-89eb-5355f6da1df4",
       "rows": [
        [
         "0",
         "Logistic Regression",
         "0.7684110748496109",
         "0.8225255972696246",
         "0.8058560553199142"
        ],
        [
         "1",
         "Decision Tree",
         "0.8012283650252275",
         "0.8875727765508934",
         "0.7445838461742209"
        ],
        [
         "2",
         "Random Forest",
         "0.8079020466002351",
         "0.8900622364986951",
         "0.8047806637765444"
        ],
        [
         "3",
         "XGBoost",
         "0.806750407745914",
         "0.8757076892190323",
         "0.8393864430363882"
        ],
        [
         "4",
         "LightGBM",
         "0.8063852951111307",
         "0.8765107408150974",
         "0.8371567731536986"
        ]
       ],
       "shape": {
        "columns": 4,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>PR-AUC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.768411</td>\n",
       "      <td>0.822526</td>\n",
       "      <td>0.805856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.801228</td>\n",
       "      <td>0.887573</td>\n",
       "      <td>0.744584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.807902</td>\n",
       "      <td>0.890062</td>\n",
       "      <td>0.804781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.806750</td>\n",
       "      <td>0.875708</td>\n",
       "      <td>0.839386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LightGBM</td>\n",
       "      <td>0.806385</td>\n",
       "      <td>0.876511</td>\n",
       "      <td>0.837157</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Model        F1  Accuracy    PR-AUC\n",
       "0  Logistic Regression  0.768411  0.822526  0.805856\n",
       "1        Decision Tree  0.801228  0.887573  0.744584\n",
       "2        Random Forest  0.807902  0.890062  0.804781\n",
       "3              XGBoost  0.806750  0.875708  0.839386\n",
       "4             LightGBM  0.806385  0.876511  0.837157"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 5 models selected: ['Random Forest', 'XGBoost', 'LightGBM', 'Decision Tree', 'Logistic Regression']\n",
      "\n",
      "Saved:\n",
      " → top5_untuned_models.joblib\n",
      " → top5_model_names.joblib\n",
      "\n",
      "BASELINE MODEL SELECTION COMPLETE.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\n\\n=== RUNNING UNTUNED BASELINE MODELS (for selection) ===\")\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.metrics import f1_score, accuracy_score, average_precision_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# Load balanced + test data\n",
    "X_train_bal = joblib.load(os.path.join(MODELS_DIR, \"X_train_bal.joblib\"))\n",
    "y_train_bal = joblib.load(os.path.join(MODELS_DIR, \"y_train_bal.joblib\"))\n",
    "X_test      = joblib.load(os.path.join(MODELS_DIR, \"X_test.joblib\"))\n",
    "y_test      = joblib.load(os.path.join(MODELS_DIR, \"y_test.joblib\"))\n",
    "\n",
    "n_classes = len(np.unique(y_train_bal))\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# DEFINE UN-TUNED MODELS\n",
    "# --------------------------------------------------------------\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=4000, class_weight=\"balanced\"),\n",
    "\n",
    "    \"Decision Tree\": DecisionTreeClassifier(),\n",
    "\n",
    "    \"Random Forest\": RandomForestClassifier(class_weight=\"balanced\"),\n",
    "\n",
    "    \"XGBoost\": XGBClassifier(\n",
    "        objective=\"multi:softprob\",\n",
    "        num_class=n_classes,\n",
    "        tree_method=\"hist\",\n",
    "        device=\"cuda\",\n",
    "        eval_metric=\"mlogloss\"\n",
    "    ),\n",
    "\n",
    "    \"LightGBM\": LGBMClassifier(\n",
    "        objective=\"multiclass\",\n",
    "        num_class=n_classes,\n",
    "        device=\"gpu\"\n",
    "    ),\n",
    "}\n",
    "\n",
    "results = []\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# TRAIN AND EVALUATE UNTUNED MODELS\n",
    "# --------------------------------------------------------------\n",
    "for name, model in models.items():\n",
    "    print(\"\\n\" + \"-\"*70)\n",
    "    print(f\"Training Untuned Model → {name}\")\n",
    "\n",
    "    model.fit(X_train_bal, y_train_bal)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    try:\n",
    "        y_proba = model.predict_proba(X_test)\n",
    "    except:\n",
    "        # some models like LinearSVC do not support predict_proba\n",
    "        y_proba = model.decision_function(X_test)\n",
    "\n",
    "    f1 = f1_score(y_test, y_pred, average=\"macro\")\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    pr_auc = average_precision_score(y_test, y_proba, average=\"macro\")\n",
    "\n",
    "    print(f\"Untuned F1: {f1:.4f} | Acc: {acc:.4f} | PR-AUC: {pr_auc:.4f}\")\n",
    "\n",
    "    results.append([name, f1, acc, pr_auc])\n",
    "\n",
    "# Convert into DataFrame\n",
    "df_results = pd.DataFrame(results, columns=[\"Model\", \"F1\", \"Accuracy\", \"PR-AUC\"])\n",
    "display(df_results)\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# SELECT TOP 4 MODELS (based on F1)\n",
    "# --------------------------------------------------------------\n",
    "top5 = df_results.sort_values(\"F1\", ascending=False).head(5)[\"Model\"].tolist()\n",
    "print(\"\\nTop 5 models selected:\", top5)\n",
    "\n",
    "# Save the actual models\n",
    "top5_models = {m: models[m] for m in top5}\n",
    "joblib.dump(top5_models, os.path.join(MODELS_DIR, \"top5_untuned_models.joblib\"))\n",
    "\n",
    "# Save the names for the next notebook\n",
    "joblib.dump(top5, os.path.join(MODELS_DIR, \"top5_model_names.joblib\"))\n",
    "\n",
    "print(\"\\nSaved:\")\n",
    "print(\" → top5_untuned_models.joblib\")\n",
    "print(\" → top5_model_names.joblib\")\n",
    "\n",
    "print(\"\\nBASELINE MODEL SELECTION COMPLETE.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
